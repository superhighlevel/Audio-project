{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrbarnSound8k_path = r\"dataset\\UrbanSound8K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"dataset\\UrbanSound8K\\metadata\\UrbanSound8K.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 'air_conditioner',\n",
    "    1: 'car_horn',\n",
    "    2: 'children_playing',\n",
    "    3: 'dog_bark',\n",
    "    4: 'drilling',\n",
    "    5: 'engine_idling',\n",
    "    6: 'gun_shot',\n",
    "    7: 'jackhammer',\n",
    "    8: 'siren',\n",
    "    9: 'street_music'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extract(file):\n",
    "    audio, sample_rate = librosa.load(file, res_type='kaiser_fast')\n",
    "    feature = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=50)\n",
    "    scaled_feature = np.mean(feature.T,axis=0)\n",
    "    return scaled_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 3555/8732 [02:14<03:14, 26.57it/s]C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  return f(*args, **kwargs)\n",
      " 95%|█████████▌| 8323/8732 [05:07<00:12, 34.01it/s]C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  return f(*args, **kwargs)\n",
      " 95%|█████████▌| 8327/8732 [05:08<00:11, 35.35it/s]C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  return f(*args, **kwargs)\n",
      "100%|██████████| 8732/8732 [05:22<00:00, 27.12it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_parsing = []\n",
    "for idx in tqdm(range(len(df))):\n",
    "    file_name = df.iloc[idx]['slice_file_name']\n",
    "    fold_number = df.iloc[idx]['fold']\n",
    "    fold = 'fold' + str(fold_number) \n",
    "    file_path = os.path.join(UrbarnSound8k_path, 'audio', fold, file_name)\n",
    "    feature_parsing.append(features_extract(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature'] = feature_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataLoader():\n",
    "    def __init__(self, data_set):\n",
    "        self.data = data_set\n",
    "        self.data_len = len(self.data)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.data.iloc[idx]['feature'])\n",
    "        label = torch.tensor(self.data.iloc[idx]['classID'])\n",
    "        label = label.long()\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_fold(valid_fold):\n",
    "    train_df = df[df['fold'] != valid_fold]\n",
    "    valid_df = df[df['fold'] == valid_fold]\n",
    "    train_loader = AudioDataLoader(train_df)\n",
    "    train_loader = DataLoader(train_loader, batch_size=32)\n",
    "    val_loader = AudioDataLoader(valid_df)\n",
    "    val_loader = DataLoader(val_loader, batch_size=32)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, drop_rate=0.2):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class SimpleAudioClassifier(nn.Module):\n",
    "    def __init__(self, num_feature, num_labels):\n",
    "        super(SimpleAudioClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.fc1 = DenseLayer(num_feature, 128)\n",
    "        self.fc2 = DenseLayer(128, 256)\n",
    "        self.fc3 = DenseLayer(256, 512)\n",
    "        self.fc4 = DenseLayer(512, 512)\n",
    "        self.fc5 = DenseLayer(512, 256)\n",
    "        self.fc6 = DenseLayer(256, 128)\n",
    "        self.fc7 = nn.Linear(128, num_labels)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(pred, label):\n",
    "    pred_label = torch.argmax(pred, 1)\n",
    "    correct = (pred_label == label).sum().item()\n",
    "    return correct / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "    for idx, (feature, label) in enumerate(data_loader):\n",
    "        feature = feature.to('cuda')\n",
    "        label = label.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = cal_accuracy(output, label)\n",
    "    return acc\n",
    "def eval(model, data_loader, optimizer, criterion):\n",
    "    for idx, (feature, label) in enumerate(data_loader):\n",
    "        feature = feature.to('cuda')\n",
    "        label = label.to('cuda')\n",
    "        output = model(feature)\n",
    "        loss = criterion(output, label)\n",
    "        acc = cal_accuracy(output, label)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_cycle(valid_fold):\n",
    "    print('Train and validate on fold {}'.format(valid_fold))\n",
    "    train_loader, valid_loader = get_dataloader_fold(valid_fold)\n",
    "    model = SimpleAudioClassifier(num_feature=50, num_labels=10).to('cuda')\n",
    "    model.apply(init_weight)\n",
    "    # print numbers of parameters\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('Number of parameters: {}'.format(params))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    count = 0\n",
    "    for epoch in tqdm(range(50)):\n",
    "        train_acc = train(model, train_loader, optimizer, criterion)\n",
    "        if train_acc >=0.9:\n",
    "            count +=1\n",
    "        else:\n",
    "            count = 0\n",
    "        if count == 3:\n",
    "            break\n",
    "    eval_acc = eval(model, valid_loader, optimizer, criterion)\n",
    "    print(eval_acc)\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validate on fold 1\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:09<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "\n",
      "\n",
      "Train and validate on fold 2\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:11<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083333333333334\n",
      "\n",
      "\n",
      "Train and validate on fold 3\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:10<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3793103448275862\n",
      "\n",
      "\n",
      "Train and validate on fold 4\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:03<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43333333333333335\n",
      "\n",
      "\n",
      "Train and validate on fold 5\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [01:59<00:07,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "\n",
      "\n",
      "Train and validate on fold 6\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:06<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5652173913043478\n",
      "\n",
      "\n",
      "Train and validate on fold 7\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:04<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "\n",
      "\n",
      "Train and validate on fold 8\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:08<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "\n",
      "\n",
      "Train and validate on fold 9\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [02:00<00:10,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4375\n",
      "\n",
      "\n",
      "Train and validate on fold 10\n",
      "Number of parameters: 599306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:07<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_total_acc = []\n",
    "for i in range(1, 11):\n",
    "    eval_total_acc.append(fold_cycle(i))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5565361069465268"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0.6666666666666666 + 0.7083333333333334 + 0.3793103448275862 + 0.43333333333333335 + 0.375 + 0.5652173913043478 + 0.3333333333333333 + 0.6666666666666666 + 0.4375 + 1.0\n",
    "m/10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55611968899e65aae44704878a441f57b22ef810619bfe4adb99f7ce8844ef9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
